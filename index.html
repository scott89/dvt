<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>reveal.js – The HTML Presentation Framework</title>

		<meta name="description" content="Deep Learning, Vision, and Others">
		<meta name="author" content="Lijun Wang">

		<meta name="apple-mobile-web-app-capable" content="yes">
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/white.css" id="theme">

		<!-- Code syntax highlighting -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>

	<body>

		<div class="reveal">

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">
                                <section data-transition="zoom" data-background="fig/deep_dream.jpg" >
                                        <br><br>
					<h1>Deep Learning, Vision, and Others</h1>
                                        <br>
                                        <h3 background-color:rgba(255,255,255,0.8)>
                                        <!--  <font color="red">Created by <a href="https://scholar.google.com/citations?user=EfTwkXMolscC&hl=en">Lijun Wang</a></font> -->
                                         Created by <a href="https://scholar.google.com/citations?user=EfTwkXMolscC&hl=en">Lijun Wang</a> 
					</h3>
				</section>
                                <section>
                                        <section>
                                                <h2> What is Deep Networks?</h2>
                                                <ul>
                                                        <li><p class="fragment"> What is neural networks (CNNs, RNNs, SAE, DBN)?</p></li>
                                                        <li><p class="fragment"> How to distinguish deep and shallow models?</p></li>
                                                </ul>
                                                <p class="fragment"> Are they deep or shallow models: <span class=fragment>SVMs, </span>
                                                <span class=fragment>three-layer CNNs, </span> 
                                                <span class=fragment>20-layer CNNs?</span></p></li>
                                        </section>
                                        <section>
                                                <h2>The Universal Appriximator Theorem [1]</h2>
                                                <blockquote>A neural network with at least one hidden layer can represent any function to an arbitary degree of
                                                accuracy so long as its hidden layer is permitted to have enough units.</blockquote>
                                                <small>[1] Hornik, Kurt, Stinchcombe, Maxwell, and White, Halbert. Multilayer feedforward networks are universal
                                                approximators. Neural Networks, 1989</small>
                                        </section>
                                        <section>
                                                <h2>So What?</h2>
                                                <ul>
                                                    <li>Shallow models: overfitting, reducing model complexity, adding regularization. </li> 
                                                    <li>Deep models: underfitting, increasing model complexity, optimization, computation resource</li>



                                                </ul>
                                                <img width=50% data-src="fig/deep_shallow.png">
                                        </section>
                                </section>
                                <section>
                                        <h2>Overview</h2><br>
                                        <ul>
                                                <li><h3>Image Classification</h3></li>
                                                <li><h3>Object Detection</h3></li>
                                                <li><h3>Dense Prediction</h3></li>
                                                <li><h3>Sequance Modeling</h3></li>
                                        </ul>
                                </section>
                                <section data-transition="zoom">
                                        <h2>Image Classification</h2>
                                        </br>
                                        <ul>
                                                <li><h3>Alex Net</h3></li>
                                                <li><h3>VGG Net</h3></li>
                                                <li><h3>GoogLe Net</h3></li>
                                                <li><h3>Deep Residual Net</h3></li>
                                        </ul>
                                        
                                </section>
                                <section>
                                        <section>
                                                <h2> Alex Net (2012, Top5: 16.4%)</h2>
                                                <p>5 Conv + 3 fully connected</p>
                                                <img width=88% data-src="fig/alex.png">
                                                <img width=88% data-src="fig/alex_res.png">
                                                
                                        </section>
                                        <section>
                                                <h2>New Technologies (Tricks)</h2>
                                                <ul>
                                                        <li>ReLU non-linear units </li>
                                                        <li>Dropout</li>
                                                        <li>Local response normalization</li>
                                                        <li>Data augmentation</li>
                                                        <li class="fragment highlight-red">1.2 million training + 50k validation</li>
                                                        <li class="fragment highlight-red">GPU training</li>
                                                </ul>
                                                
                                        </section>

                                </section>
                                <section>
                                        <section>
                                                <h2> VGG Net (2014, Top5: 7.32%)
                                                <img width = 70% data-src="fig/vgg.png">
                                        </section>
                                        <section> 
                                                <h2>New Knowledge (Feature)</h2>
                                                <ul>
                                                        <li class=fragment> Increasing depth with small (3x3) convolution filters.</li>
                                                        <ul>
                                                                <li class=fragment> A stack of two 3x3 conv layers has an effecitve receptive field of 5x5,</li>
                                                                <li class=fragment> but incorparates one additional non-linear layers,</li>
                                                                <li class=fragment> and reduces parameters</li>
                                                        </ul>
                                                        <li class=fragment> Preserve time complextiy per layer.</li>
                                                        <ul>
                                                                <li class=fragment> Layers with same output feature map size have the same number of filters.</li>
                                                                <li class=fragment> If the feature map size is halved, the number of filters is doubled.</li>
                                                        </ul>
                                                </ul>

                                        </section>
                                </section>
                                <section>
                                        <section>
                                                <h2>GoogLeNet (2014, Top5: 6.67%)</h2>
                                                <img width=98% data-src="fig/googlenet.png">
                                        </section>
                                        <section>
                                                <h2>Architecture Designs </h2>
                                                <ul>
                                                        <li class=fragment> <p>Global average pooling[2] before fully connected layers.</p></li>
                                                        <li class=fragment> <p>Multiple loss layers to enforce gradient back-propagation and encourage discrimination in lower layers.</p></li>
                                                        <li class=fragment> <p>Inceptions</p></li>
                                                </ul> <br><br><br><br><br>
                                                <small>[2] Min Lin el al. Network in network. CoRR, abs/1312.4400, 2013.</small>
                                        </section>
                                        <section>
                                                <h2> Inception </h2>
                                                <ul>
                                                        <li> Increasing feature map channels without blowing-up computational complexity.
                                                        <li> Processing signals at different scales and aggregating for abstraction. 
                                                </ul><br><br>
                                                <img width=90% data-src="fig/inception.png">
                                        </section>
                                </section>
                                <section>
                                        <section>
                                                <h2> Deep Resedual Network <small>(2015, Top5: 3.57% human-level: 5.1%)</small></h2>
                                                <blockquote>Degradation problem: with network depth increasing, accuracy gets asturated and then degrades. </blockquote>
                                                <img width=90% data-src="fig/degradation.png">
                                        </section>
                                        <section>
                                                <h2>Residual Learning </h2>
                                                <ul>
                                                        <li><p>Fit a residual mapping $\mathcal{F}(\mathbf{x}):=\mathcal{H}(\mathbf{x})-\mathbf{x}$, 
                                                        rather than fitting the original mapping $\mathcal{H}(\mathcal{x})$</p></li>
                                                        <li>It is easier to optimize the residual mapping (Considering an identity mapping as optimal).</li>
                                                </ul><br>
                                                <img width = 60% data-src="fig/res_learn.png">
                                        </section>
                                        <section>
                                                <h2>Network Architecture</h2>
                                                <img width=100% data-src="fig/res.png">
                                        </section>
                                        <section>
                                                <h2>Building Blocks</h2>
                                                <p>Identity mapping by short connect.</p>
                                                <img width=90% data-src="fig/shortconnect.png">
                                                <h3>         Naive     v.s.    Bottleneck  </3>
                                        </section>
                                        <section>
                                                <h2>Deep Residual Net Address Degradation</h2>
                                                <img width=90% data-src="fig/res_res.png">
                                        </section>
                                </section>
                                <section data-transition="zoom">
                                        <h2>Object Detection</h2>
                                        </br>
                                        <ul>
                                                <li><h3>R-CNN</h3></li>
                                                <li><h3>Fast R-CNN</h3></li>
                                                <li><h3>Faster R-CNN</h3></li>
                                                <li><h3>YOlO</h3></li>
                                        </ul>
                                </section>
                                <section>
                                        <section>
                                                <h2>R-CNN: Regions with CNN Features<h2>
                                                <img width=90% data-src="fig/rcnn.jpg">
                                        </section>
                                        <section>
                                                <h2>Pros and Cons</h2>
                                                <ul>
                                                        <li class="fragment">The first detector eploring deep CNN features with excellent performance.</li>
                                                        <li class="fragment">Training is a multi-stage pipeline</li>
                                                        <li class="fragment">Training is expensive in sapce and time</li>
                                                        <li class="fragment">Object detection is slow (47s/f)</li>
                                                </ul>
                                        </section>

                                </section>
                                <section>
                                        <section>
                                                <h2>Fast R-CNN</h2>
                                                <img width=90% data-src="fig/fast_rcnn">
                                        </section>
                                        <section>
                                                <h2>Key Features</h2>
                                                <ol>
                                                        <li class="fragment">RoI pooling layer: extracts fixed length feature for each region proposal.</li>
                                                        <li class="fragment">Multitask Learning: $L(p,u,t^u,v)=L_{cls}(p,u) + \lambda L_{loc}(t^u,v)$</li>
                                                </ol>
                                                </br><br><br>
                                                <ul>
                                                        <li class="fragment">Training is single-stage and update all network layers.</li>
                                                        <li class="fragment">No disk storage is required for feature caching.</li>
                                                </ul>
                                        </section>
                                </section>
                                <section>
                                        <section>
                                                <h2>Faster R-CNN</h2>
                                                <img width=40% data-src="fig/faster_rcnn.png">
                                                <ul>
                                                        <li>Feature Net + RPN + Fast R-CNN </li>
                                                        <li>Share deep features between object proposal and detection.</li>
                                        </section>
                                        <section>
                                                <h2>Region Proposal Net (RPN)</h2>
                                                <div>
                                                        <div class="left" style="float: left; width: 50%">
                                                                <ul>
                                                                        <li class="fragment">Slide a 3x3 filter on the convolution feature map to obtain a feature vector.</li>
                                                                        <li class="fragment">Two sibling layers: box regression (reg) and classificaion (cls)</li>
                                                                        <li class="fragment">For each window center, reg outputs 4k coordinates, cls outputs 2k class score with respect to k anchors.</li>
                                                                </ul>
                                                        </div>
                                                        <div class="right" style="float: right; width: 50%">
                                                        <br><br>
                                                                <img width=100% data-src="fig/rpn.png">
                                                        </div>
                                                </div>

                                        </section>
                                        <section>
                                                <h2>Training RPN, Fast R-CNN, and Feature Net</h2>
                                                <div>
                                                        <div class="left" style="float: left; width: 50%">
                                                                <ul>
                                                                        <li>Alternating training.</li>
                                                                        <li>Approximate joint training.</li>
                                                                        <li>Non-approximate joint training.</li>
                                                                </ul>
                                                        </div>
                                                        <div class="right" style="float: right; width: 50%">
                                                                <img width=100% data-src="fig/faster_rcnn.png">
                                                        </div>
                                                </div>

                                        </section>
                                </section>
                                <section>
                                        <section>
                                                <h2>YOLO: You Only Look Once</h2>
                                                <ul>
                                                        <li class="fragment"> Divie image into a 7x7 grid, each grid cell is responsible to detect the object within the grid.</li>
                                                        <li class="fragment"> Each cell predicts 2 bounding box and 20 class probabilities</li>
                                                        <li class="fragment"> Each bounding box includes 5 predictions: 4 relative coordinates and 1 object confidence.</li>
                                                        <li class="fragment"> Totally 7x7(2x5+20)=7x7x30 output unites.</li>
                                                </ul>
                                                <img width=50% data-src="fig/yolo.png">
                                        </section>

                                        <section>
                                                <h2>Detector Design</h2>
                                                <ul>
                                                        <li class="fragment">First 20 layers are pretrained on image classification.</li>
                                                        <li class="fragment">Increase input resolution from 224x224 to 448x448.</li>
                                                        <li class="fragment">In finetuning, each object only update 1 responsive predictior among 7x7x2 predictors.</li>
                                                </ul>
                                                <img width = 80% data-src="fig/yolo_net.png">
                                        </section>
                                        <section>
                                                <h2>Performance Comparison</h2>
                                                <img width=70% data-src="fig/pascal_2007.png">
                                        </section>
                                </section>
                                <section>
                                        <h2>Dense Prediction</h2>
                                        <ul>
                                                <li>to be continued ...</li>
                                        </ul>
                                </section>
                                <section>
                                        <h3>How people viewed networks 20 years ago</h3>
                                        <blockquote>
                                                "In spite of the seemly different underlying principles, most of the well known neural network models are implicitly equivalent or similar to classical statistical pattern recognition methods" Jain TPAMI 2000.
                                        </blockquote>
                                        <blockquote>
                                                "Neural networks are statistics for amateurs… Most neural networks conceal the statistics from the user" Anderson, 1990. 
                                        </blockquote>
                                </section>
                                <section>
                                        <h2>What's next?</h2>
                                        <ul>
                                                <li class="fragment">Simply using deep learning for vision problem is not sufficient to get our papers accepted. We are entering the post-deep-learning era. </li>
                                                <li class="fragment">We should follow most recent pregress of deep learning by reading more papers and build solid foundation on machine learning as well.</li>
                                        </ul>
                                </section>
                                <section data-transition="zoom">
                                        <h2>Dense Prediction</h2>
                                        </br>
                                        <ul>
                                                <li><h3>Structured Learning</h3></li>
                                                <li><h3>Weakly- and Semi- Supervised Learning</h3></li>
                                        </ul>
                                </section>

                                <section>
                                        <section>
                                                <h3><a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Pathak_Constrained_Convolutional_Neural_ICCV_2015_paper.pdf">
						Constrained Convolutional Neural Networks for Weakly Supervised Segmentation</a></h3>
                                                <h6>Deepak Pathak, 
						<a href=http://www.philkr.net/home/>Philipp Krähenbühl</a>, 
						<a href=http://bvlc.eecs.berkeley.edu/>Trevor Darrell</a></h6>
                                                <h6>UC Berkeley</h6>
                                                <img width=45% data-src="fig/ccnn.png">
                                        </section>        
                                        <section>
                                                <h2>Semantic Segmentation</h2>
                                                <p class="fragment">Given image I, predict pixel labels $X=\{x_0,\ldots,x_n\}$.</p>
                                                <p class="fragment">CNN models the distribution by $Q(X|\theta,I)=\prod_i q_i(x_i|\theta,I)$,</p>
                                                <p class="fragment">where $q_i(x_i|\theta,I)=\frac{1}{Z_i}\exp(f_i(x_i;\theta,I))$</p>
                                        </section>
                                        <section>
                                                <h2>Fully and Weakly Supervised Segmentation</h2>
                                                <p class="fragment">For fully supervised training:</p>
                                                <pc class="fragment">$\arg \max\limits_{\theta} \sum_{I} Q(X|\theta, I)$</pc>
                                                <br>
                                                <p class="fragment"> For weakly supervised training:</p>
                                                <pc class="fragment"> find $\theta$  <br>
                                                                     subject to $A_i Q_i \geq b_I  \quad \forall I$</pc>
                                        </section>
                                        <section>
                                                <h2>Weakly Supervised Segmentation</h2>
                                                <img width=70% data-src="fig/constrained_training.png">
                                                <p class="fragment"> However it is hard to directly optimize</p>
                                                <pc class="fragment"> find $\theta$  <br>
                                                                     subject to $A_I Q_I \geq b_I  \quad \forall I$</pc>
                                        </section>
                                        <section>
                                                <h2>Constrained Optimization</h2>
                                                <p text-align="center"> Introduce a latent probability distribution $P(X)$:</p>
                                                    <br>
                                                <pc>
                                                    $\min \limits_{\theta,P} D(P(X)\|Q(X|\theta))$
                                                    <br>
                                                    subject to $AP \geq b, \quad \sum \limits_{X} P(X)=1$,
                                                </pc>
                                                <p>where KL-divergence $D(p(x)\|q(x))=\sum_x p(x)\log \frac{p(x)}{q(x)}$ measures the distance of two distributions.</p>
                                        </section>
                                        <section>
                                                <h2>Constraints for Weak Segmentation</h2>
                                                <p><b>Suppresion constraint</b> suppress any label $l$ that does not appear in the image:</p>
                                                <pc>$\sum \limits_{i=1}^{n} p_i(l) \leq 0 \quad \forall l \notin \mathcal{L}_I$</pc>
                                        </section>
                                        <section>
                                                <h2>Constraints for Weak Segmentation</h2>
                                                <p><b>Foreground constraint</b> encourages foreground:</p>
                                                <pc>$\sum \limits_{i=1}^{n} p_i(l) \geq a_l \quad \forall l \in \mathcal{L}_I$</pc>
                                                <p> Compare with multiple instance learning (MIL) paradigm
                                        </section>
                                        <section>
                                                <h2>Constraints for Weak Segmentation</h2>
                                                <p><b>Background constraint</b> constrain background regions</p>
                                                <pc>$a_0 \leq \sum \limits_{i=1}^{n} p_i(0) \leq b_0.$</pc>
                                        </section>
                                        <section>
                                                <h2>Constraints for Weak Segmentation</h2>
                                                <p><b>Size constraint</b> put an upper bound constraint on classes that are guaranteed to be small:</p>
                                                <pc>$\sum \limits_{i=1}^{n} p_i(l) \leq b_l.$</pc>
                                        </section>
                                        <section>
                                                <h2>Experiments</h2>
                                                <p>VGG + Fully Connected CRF. Constrained Optimization is performed on course maps generated by VGG.</p>
                                                <img width=100% data-src="fig/weak_comp.png">
                                        </section>
                                        <section>
                                                <h2>Experiments</h2>
                                                <p>VGG + Fully Connected CRF. Constrained Optimization is performed on course maps generated by VGG.</p>
                                                <img width=100% data-src="fig/full_comp.png">
                                        </section>
                                </section>
                                <section>
                                        <section>
                                               <h3><a href=http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Papandreou_Weakly-_and_Semi-Supervised_ICCV_2015_paper.pdf>
					       Weakly- and Semi-supervised Learning of DNN for Semantic Segmentaiton</a></h3> 
                                               <table>
                                                        <tbody>
                                                                <tr> <td border="none">G. Papandreou</td>
								<td border="none">L. Chen</td>
								<td border="none">K. Murphy</td>
								<td border="none"><a href="http://www.stat.ucla.edu/~yuille/">
								 A. Yuille</a></td></tr>
                                                                <tr> <td border="none">Google, Inc.</td>
								<td border="none">UCLA</td>
								<td border="none">Google, Inc.</td>
								<td border="none">UCLA</td></tr>
                                                        </tbody>
                                               </table>
					       <br><br>
					       <ul>
					       		<li>Pixel-level annotations</li>
					       		<li>Image-level annotations</li>
					       		<li>Bounding box annotations</li>
					       </ul>
                                        </section>
					<section>
						<h2>Image-level Annotations</h2>
						<p>Formulate training as hard-EM approximation, with compete-data log likelihood:</p>
						<pc>$Q(\theta;\theta^{'})=\sum \limits_{Y} P(Y|X,z;\theta^{'}) \log P(Y|X;\theta)$ <br> $\approx \log(\hat{Y}|X;\theta)$</pc>
					</section>
					<section>
						<h2>Image-level Annotations</h2>
						<p> E-step: update the latent segmentation</p>
						<pc>$\hat{Y}=\arg \max \limits_{Y} P(Y,X;\theta^{'})P(z|Y)$</pc>
						<br><br>
						<p> M-step: minimize $Q(\theta;\theta^{'})$ using stochastic gradient descent.</p>
						
					</section>
					<section>
						<h2>Image-level Annotations</h2>
						<p>In summary:</p>
						<ol>
							<li class="fragment">Refine latend segmentation $\hat{Y}$ with current network output and image label constraint.</li>
							<br><br>
							<li class="fragment">Train the network with refined segmentation $\hat{Y}$ as GT</li>
						</ol>
					</section>
					<section>
						<h2>Bounding Box Annotations</h2>
					</section>
					<section>
						<h2>Mixed Strong and Weak Annotations</h2>
					</section>
                                </section>
				<section>
					<h2></h2>
				</section>
			</div>

		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>

			// Full list of configuration options available at:
			// https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: false,

				transition: 'slide', // none/fade/slide/convex/concave/zoom

				// Optional reveal.js plugins
				dependencies: [
					{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'plugin/zoom-js/zoom.js', async: true },
					{ src: 'plugin/notes/notes.js', async: true },
                                        { src: 'plugin/math/math.js', async: true}
				]
			});

		</script>

	</body>
</html>
